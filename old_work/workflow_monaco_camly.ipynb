{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functions import *\n",
    "from datetime import datetime, timedelta, time\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/media/clrintz/plancha_drive_3/plancha_session/data/\"\n",
    "SESSION_NAME = \"session_2022_10_19_aldabra_ARM01_plancha_body_v1A_00\"\n",
    "\n",
    "# La Reunion only\n",
    "# rgp_station = \"lepo\"\n",
    "\n",
    "frames_per_second = \"3\"\n",
    "\n",
    "leap_sec = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived parameters\n",
    "exiftool_config_path = \"/home/clrintz/Documents/workflow_plancha/exiftool_roll_pitch_config_file.config\"\n",
    "PPK_CONFIG_PATH = \"/home/clrintz/Documents/workflow_plancha/\"  # location of config files\n",
    "ppk_cfgs = [\"ppk_config_file\"]  # list of config files to run (files should have .conf ext)\n",
    "\n",
    "SESSION_PATH = ROOT + SESSION_NAME\n",
    "VIDEOS_PATH = SESSION_PATH + \"/DCIM/videos\"\n",
    "IMAGES_PATH = SESSION_PATH + \"/DCIM/images\"\n",
    "FRAMES_PATH = VIDEOS_PATH + \"/frames\"\n",
    "GPS_PATH = SESSION_PATH + \"/GPS\"\n",
    "GPS_BASE_PATH = GPS_PATH + \"/BASE\"\n",
    "GPS_DEVICE_PATH = GPS_PATH + \"/DEVICE\"\n",
    "BATHY_PATH = SESSION_PATH + \"/BATHY\"\n",
    "#METADATA_PATH = SESSION_PATH + \"/METADATA/\" + SESSION_NAME + \"_exif_metadata.csv\"\n",
    "METADATA_PATH = SESSION_PATH + \"/METADATA\"\n",
    "CSV_EXIFTOOL_FRAMES = METADATA_PATH + \"/metadata.csv\"\n",
    "CSV_EXIFTOOL_VIDEO =  METADATA_PATH + \"/csv_exiftool_video.csv\"\n",
    "delta_time = str(1/float(frames_per_second))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIM VIDEOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_videos(VIDEOS_PATH, FRAMES_PATH, frames_per_second, SESSION_NAME, METADATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGE IMAGES COLORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image, ImageOps\n",
    "PIL_PATH = FRAMES_PATH\n",
    "for file in os.listdir(PIL_PATH):\n",
    "    im_path = PIL_PATH + \"/\" + file\n",
    "    im = Image.open(im_path)\n",
    "    r, g, b = im.split()\n",
    "    r, g, b = ImageOps.autocontrast(r, cutoff = 1), ImageOps.autocontrast(g, cutoff = 1), ImageOps.autocontrast(b, cutoff = 1)\n",
    "    im = Image.merge(\"RGB\",[r, g, b])\n",
    "    im.save(im_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME CALIBRATE AND GEOTAG IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date and time of the first frame are :  2022:10:19 11:56:00.400000\n",
      "##############################################################################\n",
      "WRITE SESSION_INFO CSV\n",
      "##############################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parameter definition\n",
    "# N.B. insert the date and time following the format \"YYYY:MM:DD HH:MM:SS.000\"\n",
    "# N.B. insert the time in UTC+0 format\n",
    "time_first_frame = \"2022:10:19 11:56:00.400\"\n",
    "\n",
    "# correct time by adding leap seconds to match GPS time (in 2023 it is 18 s)\n",
    "d_date = datetime.strptime(time_first_frame , '%Y:%m:%d %H:%M:%S.%f') + pd.Timedelta(seconds = leap_sec)\n",
    "time_first_frame = d_date.strftime('%Y:%m:%d %H:%M:%S.%f')\n",
    "\n",
    "print(\"The date and time of the first frame are : \", time_first_frame)\n",
    "print(\"##############################################################################\")\n",
    "print(\"WRITE SESSION_INFO CSV\")\n",
    "print(\"##############################################################################\\n\")\n",
    "# open csv file if exist, if not create it\n",
    "SESSION_INFO_PATH = METADATA_PATH +\"/session_info.csv\"\n",
    "if os.path.exists(SESSION_INFO_PATH):\n",
    "   session_info = pd.read_csv(SESSION_INFO_PATH)\n",
    "else:\n",
    "   session_info = pd.DataFrame([frames_per_second, time_first_frame, leap_sec]).T\n",
    "   \n",
    "# add parameters to df\n",
    "session_info[\"frames_per_second\"] = frames_per_second\n",
    "session_info[\"time_first_frame\"] = time_first_frame\n",
    "session_info[\"leap_sec\"] = leap_sec\n",
    "#session_info.pop(session_info.columns[0])\n",
    "\n",
    "# save df\n",
    "session_info.to_csv(SESSION_INFO_PATH, sep = ',', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We already have a GPS file with PPK solution\n",
      "/media/clrintz/plancha_drive_3/plancha_session/data/session_2022_10_19_aldabra_ARM01_plancha_body_v1A_00/GPS/DEVICE/ppk_solution_session_2022_10_19_aldabra_ARM01_plancha_body_v1A_00.LLH\n",
      "The NEW navigation file will be :  /media/clrintz/plancha_drive_3/plancha_session/data/session_2022_10_19_aldabra_ARM01_plancha_body_v1A_00/GPS/DEVICE/ppk_solution_session_2022_10_19_aldabra_ARM01_plancha_body_v1A_00.txt\n"
     ]
    }
   ],
   "source": [
    "flag_gps = 0\n",
    "flag_device = 0   # old name = flag_gps\n",
    "flag_base = 0   # 1 : RINEX ; 2 : RGP\n",
    "\n",
    "# ----- Check if we need to process PPK solution \n",
    "\n",
    "for file in os.listdir(GPS_DEVICE_PATH) :\n",
    "    if (\"ppk_solution\" in file) and (file.endswith(\".LLH\")):\n",
    "        print(\"We already have a GPS file with PPK solution\")\n",
    "        LLH_PATH = GPS_DEVICE_PATH + \"/\" + file\n",
    "        flag_gps = 1\n",
    "\n",
    "\n",
    "if (flag_gps == 0) :\n",
    "# ----- Check if we can process PPK solution \n",
    "\n",
    "    # 1- If we have a device RINEX folder\n",
    "    for folder in os.listdir(GPS_DEVICE_PATH) :\n",
    "        if (\"RINEX\" in folder) :\n",
    "            flag_device = 1\n",
    "\n",
    "\n",
    "    # 2- If we have base GPS data\n",
    "    if os.path.exists(GPS_BASE_PATH) :\n",
    "        # 2.a- If we have a base RINEX folder\n",
    "        for folder in os.listdir(GPS_BASE_PATH) :\n",
    "            if (\"RINEX\" in folder) :\n",
    "                flag_base = 1\n",
    "\n",
    "        # 2.b- If we don't have a RINEX folder, look for RGP files\n",
    "        if (flag_base == 0) :\n",
    "            for file in os.listdir(GPS_BASE_PATH) :\n",
    "                # 2.b.a- If we have data from RGP station\n",
    "                if file.endswith(\"d\") :\n",
    "                    flag_base = 2\n",
    "\n",
    "        # 2.b.b- If we don't have data from RGP station, download it\n",
    "        if (flag_base == 0) :\n",
    "            print(\"Downloading RGP data from\", rgp_station, \"station :\")\n",
    "            flag_base = download_rgp(SESSION_NAME, time_first_frame, FRAMES_PATH, GPS_BASE_PATH, rgp_station, delta_time)\n",
    "\n",
    "    if (flag_base != 0) and (flag_device != 0) :\n",
    "        print(\"we can do PPK on our data !\")\n",
    "        LLH_PATH = ppk(SESSION_NAME, GPS_BASE_PATH, GPS_DEVICE_PATH, PPK_CONFIG_PATH, ppk_cfgs, flag_base)\n",
    "        flag_gps = 1\n",
    "\n",
    "\n",
    "# ----- If we cannot process PPK solution\n",
    "if (flag_gps == 0) :\n",
    "    print(\"we cannot do PPK on our data at the moment !\")\n",
    "    \n",
    "    # Look for the LLH folder\n",
    "    for folder in os.listdir(GPS_DEVICE_PATH) :\n",
    "        if (\"LLH\" in folder) :\n",
    "\n",
    "            LLH_FOLDER_PATH = GPS_DEVICE_PATH + \"/\" + folder.replace(\".zip\", \"\") + \"/\"\n",
    "            \n",
    "            # If it is a zipped folder, unzip it\n",
    "            if folder.endswith(\".zip\") :\n",
    "                \n",
    "                with zipfile.ZipFile(GPS_DEVICE_PATH + \"/\" + folder, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(LLH_FOLDER_PATH)\n",
    "                break\n",
    "    \n",
    "    # Get the gps file\n",
    "    for file in os.listdir(LLH_FOLDER_PATH):\n",
    "        if file.endswith(\".LLH\"):\n",
    "            flag_gps = 1\n",
    "            LLH_PATH = LLH_FOLDER_PATH + \"/\" + file\n",
    "            break\n",
    "\n",
    "# ----- Get the final GPS file with or without PPK solution\n",
    "# Check if we have a GPS file\n",
    "if flag_gps == 1 :\n",
    "    print(os.path.join(\"We have the following navigation file : \", LLH_PATH))\n",
    "    TXT_PATH  = llh_to_txt(LLH_PATH)\n",
    "    print(\"The NEW navigation file will be : \", TXT_PATH)\n",
    "else :\n",
    "    print(\"We do not have a navigation file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_bathy = 0\n",
    "\n",
    "if os.path.isdir(BATHY_PATH) :\n",
    "    for file in os.listdir(BATHY_PATH):\n",
    "        if file.endswith(\"bathy_preproc.csv\"):\n",
    "            flag_bathy = 1\n",
    "            BATHY_PREPROC_PATH = BATHY_PATH + \"/\" + file\n",
    "            CSV_BATHY_PREPOC = bathy_preproc_to_txt(BATHY_PREPROC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "1 of 4 : EXPORT VIDEO & FRAME METADATA TO CSV\n",
      "##############################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    1 directories scanned\n",
      " 9002 image files read\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "2 of 4 : ADD DATE AND TIME TO CSV METADATA\n",
      "##############################################################################\n",
      "\n",
      "##############################################################################\n",
      "3 of 4 : ADD POSITION, ROLL, PITCH, YAW, DEPTH TO CSV METADATA\n",
      "##############################################################################\n",
      "\n",
      "##############################################################################\n",
      "4 of 4 : IMPORT EXIF METADATA\n",
      "##############################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    1 directories scanned\n",
      " 9002 image files updated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##############################################################################\")\n",
    "print(\"1 of 4 : EXPORT VIDEO & FRAME METADATA TO CSV\")\n",
    "print(\"##############################################################################\\n\")\n",
    "CSV_EXIFTOOL_FRAMES = METADATA_PATH + \"/metadata.csv\"\n",
    "CSV_EXIFTOOL_VIDEO =  METADATA_PATH + \"/csv_exiftool_video.csv\"\n",
    "export_frame_metadata =  \"exiftool -csv -fileorder filename \" + FRAMES_PATH + \" > \" + CSV_EXIFTOOL_FRAMES\n",
    "os.system(export_frame_metadata)\n",
    "\n",
    "# import frames metadata\n",
    "csv_exiftool_frames = pd.read_csv(CSV_EXIFTOOL_FRAMES)\n",
    "# import video metadata\n",
    "if os.path.isdir(VIDEOS_PATH) :\n",
    "    # for each file in the videos folder\n",
    "    for file in os.listdir(VIDEOS_PATH):\n",
    "        if file.endswith(\".MP4\") or file.endswith(\".mp4\"):    \n",
    "            CSV_EXIFTOOL_VIDEO =  METADATA_PATH + \"/csv_exiftool_video.csv\"\n",
    "            export_video_metadata =  \"exiftool -csv  \" + VIDEOS_PATH + \"/\" + file + \" > \" + CSV_EXIFTOOL_VIDEO\n",
    "            os.system(export_video_metadata)\n",
    "            break\n",
    "csv_exiftool_video = pd.read_csv(CSV_EXIFTOOL_VIDEO)\n",
    "# filter video metadata\n",
    "useful_video_metadata_names = ['LensSerialNumber', 'CameraSerialNumber', 'Model', 'AutoRotation', 'DigitalZoom', 'ProTune', 'WhiteBalance', 'Sharpness', 'ColorMode', 'MaximumShutterAngle', 'AutoISOMax', 'AutoISOMin', 'ExposureCompensation', 'Rate', 'FieldOfView', 'ElectronicImageStabilization', 'ImageWidth', 'ImageHeight', 'SourceImageHeight', 'XResolution', 'VideoFrameRate', 'ImageSize',\t'Megapixels', 'AvgBitrate']\n",
    "\n",
    "video_col_names = csv_exiftool_video.columns\n",
    "video_intersection_list = list(set(video_col_names) & set(useful_video_metadata_names))\n",
    "csv_exiftool_video = csv_exiftool_video[video_intersection_list]\n",
    "useful_video_metadata_values = csv_exiftool_video.iloc[0]\n",
    "# write video's metadata to frame csv\n",
    "for i in range(len(video_intersection_list)):\n",
    "    csv_exiftool_frames[video_intersection_list[i]] = useful_video_metadata_values[i]\n",
    "# concat session_info csv and csv_exiftool_video csv\n",
    "result = pd.concat([session_info, csv_exiftool_video], axis=1)\n",
    "result.to_csv(SESSION_INFO_PATH, sep = ',', index=False)\n",
    "# then remove csv_exiftool_video csv\n",
    "os.remove(CSV_EXIFTOOL_VIDEO)\t\n",
    "print(\"##############################################################################\")\n",
    "print(\"2 of 4 : ADD DATE AND TIME TO CSV METADATA\")\n",
    "print(\"##############################################################################\\n\")\n",
    "# convert \"time_first_frame\" to \"time_first_frame_np\" in order to create np vector of DateTime\n",
    "time_first_frame_np = time_first_frame.replace(\" \", \"T\")\n",
    "time_first_frame_np = time_first_frame_np.replace(\":\", \"-\", 2)\n",
    "# define time first frame in np format, step and nb of samples\n",
    "start = np.datetime64(time_first_frame_np)\n",
    "step = np.timedelta64(int(1/float(frames_per_second)*1000), \"ms\")\n",
    "nb_of_frames = csv_exiftool_frames.shape[0] \n",
    "# create vector of dates and times\n",
    "datetime_vec_np = np.arange(0,nb_of_frames)*step+start\n",
    "datetime_vec = []\n",
    "# convert \"datetime_vec_np\" to \"datetime_vec\" in order to create vector of DateTime in Exiftool format\n",
    "for curr_datetime in datetime_vec_np :\n",
    "    curr_datetime = str(curr_datetime)\n",
    "    curr_datetime = curr_datetime.replace(\"T\", \" \")\n",
    "    curr_datetime = curr_datetime.replace(\"-\", \":\", 2)\n",
    "    datetime_vec.append(curr_datetime)\n",
    "csv_exiftool_frames[\"SubSecDateTimeOriginal\"] = datetime_vec\n",
    "csv_exiftool_frames[\"SubSecDateTimeOriginal_np\"] = datetime_vec_np\n",
    "\n",
    "\n",
    "if flag_gps == 1 :\n",
    "    print(\"##############################################################################\")\n",
    "    print(\"3 of 4 : ADD POSITION, ROLL, PITCH, YAW, DEPTH TO CSV METADATA\")\n",
    "    print(\"##############################################################################\\n\")\n",
    "    # convert \"SubSecDateTimeOriginal_np\" to unix time in order to do interpolation\n",
    "    # please see : https://www.unixtimestamp.com/\n",
    "    csv_exiftool_frames['datetime_unix'] = csv_exiftool_frames['SubSecDateTimeOriginal_np'].astype('int64')\n",
    "    #############################\n",
    "    # import lat and lon from LLH\n",
    "    #############################\n",
    "    #LLH_PATH = \"/home/mcontini/Desktop/PhD/Git_projects/plancha/prova/reach_raw_202210200948_b33_ppk.txt\"\n",
    "    csv_llh = pd.read_csv(TXT_PATH)\n",
    "    # create datetime col\n",
    "    csv_llh['SubSecDateTimeOriginal_np'] = csv_llh['GPSDateStamp'] + ' ' + csv_llh['GPSTimeStamp']\n",
    "    # adapt format to the exiftool one\n",
    "    csv_llh['SubSecDateTimeOriginal_np'] = csv_llh['SubSecDateTimeOriginal_np'].str.replace(\"/\", \"-\")\n",
    "    # convert column to date type\n",
    "    csv_llh['SubSecDateTimeOriginal_np'] = pd.to_datetime(csv_llh['SubSecDateTimeOriginal_np'])\n",
    "    csv_llh['datetime_unix'] = csv_llh['SubSecDateTimeOriginal_np'].values.astype('int64')\n",
    "    # linear interpolation, if different interpolation needed :\n",
    "    # please see :\n",
    "    # https://docs.scipy.org/doc/scipy/tutorial/interpolate/1D.html\n",
    "    csv_exiftool_frames['GPSLatitude'] = np.interp(csv_exiftool_frames['datetime_unix'], csv_llh['datetime_unix'], csv_llh['GPSLatitude'])\n",
    "    csv_exiftool_frames['GPSLongitude'] = np.interp(csv_exiftool_frames['datetime_unix'], csv_llh['datetime_unix'], csv_llh['GPSLongitude'])\n",
    "    # we want to write lat and lon in \"Composite\" family tags, because in \"Exif\" family tags we cannot assign \"-\" sign to lat and lon\n",
    "    csv_exiftool_frames.rename(columns={\"GPSLatitude\": \"Composite:GPSLatitude\", \"GPSLongitude\": \"Composite:GPSLongitude\"}, inplace=True)\n",
    "\n",
    "if flag_bathy == 1 :\n",
    "    ######################################################\n",
    "    # import roll, pitch, yaw and depth from bathy_preproc\n",
    "    ######################################################\n",
    "\n",
    "    csv_bathy_preproc = pd.read_csv(CSV_BATHY_PREPOC)\n",
    "    # delete 3 last digit of \"GPS_time\"\n",
    "    csv_bathy_preproc['GPS_time'] = csv_bathy_preproc['GPS_time'].str[:-3]\n",
    "    # adapt format to the exiftool one\n",
    "    csv_bathy_preproc['GPS_time'] = pd.to_datetime(csv_bathy_preproc['GPS_time'])\n",
    "    csv_bathy_preproc['datetime_unix'] = csv_bathy_preproc['GPS_time'].values.astype('int64')\n",
    "\n",
    "    csv_exiftool_frames['XMP:GPSRoll'] = np.interp(csv_exiftool_frames['datetime_unix'], csv_bathy_preproc['datetime_unix'], csv_bathy_preproc['GPSRoll'])\n",
    "    csv_exiftool_frames['XMP:GPSPitch'] = np.interp(csv_exiftool_frames['datetime_unix'], csv_bathy_preproc['datetime_unix'], csv_bathy_preproc['GPSPitch'])\n",
    "    csv_exiftool_frames['XMP:GPSTrack'] = np.interp(csv_exiftool_frames['datetime_unix'], csv_bathy_preproc['datetime_unix'], csv_bathy_preproc['GPSTrack'])\n",
    "    csv_exiftool_frames['GPSAltitude'] = np.interp(csv_exiftool_frames['datetime_unix'], csv_bathy_preproc['datetime_unix'], csv_bathy_preproc['GPSAltitude'])\n",
    "    # delete GPS:Position column\n",
    "    #csv_exiftool_frames = csv_exiftool_frames.drop('GPSPosition', axis=1)\n",
    "    \n",
    "    # set altitude below sea level\n",
    "    csv_exiftool_frames['GPSAltitudeRef'] = \"Below Sea Level\"\n",
    "\n",
    "# add useful GoPro metadata\n",
    "if csv_exiftool_frames['FieldOfView'][0]==\"Linear\" :\n",
    "    csv_exiftool_frames['EXIF:FocalLength'] = \"2.92\"\n",
    "    csv_exiftool_frames[\"EXIF:FocalLengthIn35mmFormat\"] = \"15\"\n",
    "\n",
    "print(\"##############################################################################\")\n",
    "print(\"4 of 4 : IMPORT EXIF METADATA\")\n",
    "print(\"##############################################################################\\n\")\n",
    "# save frame csv, before import metadata\n",
    "csv_exiftool_frames.to_csv(CSV_EXIFTOOL_FRAMES, index=False)\n",
    "\n",
    "# CLR 07/07/2023 ---- Comment the next 2 lines so that we don't geotag the frames --> faster ! ----------\n",
    "import_csv_metadata =  \"exiftool -config \" + exiftool_config_path + \" -csv=\" + CSV_EXIFTOOL_FRAMES + \" -fileorder filename \" + FRAMES_PATH + \" -overwrite_original\"\n",
    "os.system(import_csv_metadata)\n",
    "\n",
    "# once we have imported all metadata, remove useless columns from metadata csv\n",
    "col_names = csv_exiftool_frames.columns\n",
    "# EXIF metadata we want to keep, please check :\n",
    "# https://docs.google.com/spreadsheets/d/1iSKDvFrh-kP9wOU9bt9H7lcZKOnF7pe9n-8t15pOrmw/edit?usp=sharing\n",
    "keep_param_list = [\"ApertureValue\", \"Compression\", \"Contrast\", \"CreateDate\", \"DateCreated\", \"DateTimeDigitized\", \"DateTimeOriginal\", \"DigitalZoomRatio\", \"ExifImageHeight\", \"ExifImageWidth\", \n",
    "                    \"ExifToolVersion\", \"ExifVersion\", \"ExposureCompensation\", \"ExposureMode\", \"ExposureProgram\", \"FileName\", \"FileSize\", \"FileType\", \"FileTypeExtension\", \"FNumber\", \n",
    "                    \"FocalLength\", \"FocalLength35efl\", \"FocalLengthIn35mmFormat\", \"FOV\", \"GPSAltitude\", \"GPSAltitudeRef\", \"GPSDateTime\", \"GPSDate\", \"GPSTime\", \"GPSLatitude\", \"GPSLatitudeRef\", \"GPSLongitude\", \n",
    "                    \"GPSLongitudeRef\", \"GPSMapDatum\", \"GPSPosition\", \"GPSTimeStamp\", \"GPSRoll\", \"GPSPitch\", \"GPSTrack\", \"ImageHeight\", \"ImageWidth\", \"LightValue\", \"Make\", \"MaxApertureValue\", \n",
    "                    \"MaximumShutterAngle\", \"Megapixels\", \"MeteringMode\", \"MIMEType\", \"Model\", \"Saturation\", \"ScaleFactor35efl\", \"SceneCaptureType\", \"SceneType\", \"SensingMethod\", \"Sharpness\", \n",
    "                    \"ShutterSpeed\", \"Software\", \"SubSecDateTimeOriginal\", \"ThumbnailImage\", \"ThumbnailLength\", \"ThumbnailOffset\", \"WhiteBalance\", \"XResolution\", \"YResolution\", \"Composite:GPSLatitude\", \"Composite:GPSLongitude\"]\n",
    "# intersection between metadata we want to keep and EXIF metadata\n",
    "intersection_list = list(set(col_names) & set(keep_param_list))\n",
    "# filter df\n",
    "csv_exiftool_frames = csv_exiftool_frames[intersection_list]\n",
    "# delete all empty columns\n",
    "csv_exiftool_frames.dropna(axis=1,inplace=True)\n",
    "# delete all zero columns\n",
    "csv_exiftool_frames = csv_exiftool_frames.loc[:, (csv_exiftool_frames != 0).any(axis=0)]\n",
    "# delete useless col\n",
    "#csv_exiftool_frames.drop(\"SubSecDateTimeOriginal_np\", axis=1, inplace=True)\n",
    "\n",
    "# save filtered frame csv, after import metadata\n",
    "csv_exiftool_frames.to_csv(CSV_EXIFTOOL_FRAMES, index=False)\n",
    "\n",
    "# end message\n",
    "os.system('spd-say \"geotagging frames is done\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT CURRENT SESSION IN NEW ARBORESCENCE FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN_ROOT = \"/media/mcontini/plancha_drive_32/plancha_session/data\"\n",
    "DEST_ROOT = \"/media/mcontini/Data_Paper_Hard_Disk/data\"\n",
    "# If it does not already exist, create a new folder\n",
    "data_folder_name = os.path.basename(os.path.normpath(DEST_ROOT))\n",
    "NEW_DEST_ROOT =  os.path.dirname(DEST_ROOT) + \"/\" + data_folder_name + \"_new\"\n",
    "\n",
    "# we will treat the following root\n",
    "if not os.path.exists(NEW_DEST_ROOT):\n",
    "    # create new root folder\n",
    "    print(\"we are creating the following folder : \\n\", NEW_ROOT)\n",
    "    os.makedirs(NEW_DEST_ROOT)\n",
    "\n",
    "\n",
    "NEW_SESSION_PATH = NEW_DEST_ROOT + \"/\" + SESSION_NAME\n",
    "\n",
    "# if NEW_SESSION_PATH folder does not exist, create it with all subfolders\n",
    "if not os.path.exists(NEW_SESSION_PATH):\n",
    "    create_new_session_folder(ORIGIN_ROOT, NEW_DEST_ROOT, SESSION_NAME)\n",
    "\n",
    "else :\n",
    "    print(\"we have already treated the session : \\n\", NEW_SESSION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rename images old names\n",
    "os.system(\"rename 's/pascal_20151210_PAER/hermitage_mask_v1A/g' \" + IMAGES_PATH + \"/*.JPG\")\n",
    "CSV_EXIFTOOL_FRAMES = METADATA_PATH + \"/metadata.csv\"\n",
    "export_frame_metadata =  \"exiftool -csv -fileorder filename \" + IMAGES_PATH + \" > \" + CSV_EXIFTOOL_FRAMES\n",
    "os.system(export_frame_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv_exiftool_frames = pd.read_csv(CSV_EXIFTOOL_FRAMES)\n",
    "#import_csv_metadata =  \"exiftool -csv=\" + CSV_EXIFTOOL + \" -fileorder filename \" + FRAMES_PATH + \" -c '%.9f'\"\n",
    "\n",
    "# once we have imported all metadata, remove useless columns from metadata csv\n",
    "col_names = csv_exiftool_frames.columns\n",
    "# EXIF metadata we want to keep, please check :\n",
    "# https://docs.google.com/spreadsheets/d/1iSKDvFrh-kP9wOU9bt9H7lcZKOnF7pe9n-8t15pOrmw/edit?usp=sharing\n",
    "keep_param_list = [\"ApertureValue\", \"Compression\", \"Contrast\", \"CreateDate\", \"DateCreated\", \"DateTimeDigitized\", \"DateTimeOriginal\", \"DigitalZoomRatio\", \"ExifImageHeight\", \"ExifImageWidth\", \n",
    "                    \"ExifToolVersion\", \"ExifVersion\", \"ExposureCompensation\", \"ExposureMode\", \"ExposureProgram\", \"FileName\", \"FileSize\", \"FileType\", \"FileTypeExtension\", \"FNumber\", \n",
    "                    \"FocalLength\", \"FocalLength35efl\", \"FocalLengthIn35mmFormat\", \"FOV\", \"GPSAltitude\", \"GPSAltitudeRef\", \"GPSDateTime\", \"GPSDate\", \"GPSTime\", \"GPSLatitude\", \"GPSLatitudeRef\", \"GPSLongitude\", \n",
    "                    \"GPSLongitudeRef\", \"GPSMapDatum\", \"GPSPosition\", \"GPSTimeStamp\", \"GPSRoll\", \"GPSPitch\", \"GPSTrack\",\"ImageHeight\", \"ImageWidth\", \"LightValue\", \"Make\", \"MaxApertureValue\", \n",
    "                    \"MaximumShutterAngle\", \"Megapixels\", \"MeteringMode\", \"MIMEType\", \"Model\", \"Saturation\", \"ScaleFactor35efl\", \"SceneCaptureType\", \"SceneType\", \"SensingMethod\", \"Sharpness\", \n",
    "                    \"ShutterSpeed\", \"Software\", \"SubSecDateTimeOriginal\", \"ThumbnailImage\", \"ThumbnailLength\", \"ThumbnailOffset\", \"WhiteBalance\", \"XResolution\", \"YResolution\"]\n",
    "# intersection between etadata we want to keep and EXIF meatdata\n",
    "intersection_list = list(set(col_names) & set(keep_param_list))\n",
    "# filter df\n",
    "csv_exiftool_frames = csv_exiftool_frames[intersection_list]\n",
    "# delete all empty columns\n",
    "csv_exiftool_frames.dropna(axis=1,inplace=True)\n",
    "# delete all zero columns\n",
    "csv_exiftool_frames = csv_exiftool_frames.loc[:, (csv_exiftool_frames != 0).any(axis=0)]\n",
    "# delete useless col\n",
    "#csv_exiftool_frames.drop(\"SubSecDateTimeOriginal_np\", axis=1, inplace=True)\n",
    "# save filtered frame csv, after import metadata\n",
    "csv_exiftool_frames.to_csv(CSV_EXIFTOOL_FRAMES, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGES DEJA ANNOTEES\n",
    "# set correct time\n",
    "os.system(\"exiftool -m '-SubSecTimeOriginal<GPSDateTime'  '-SubSecTime<GPSDateTime' '-SubSecTimeDigitized<GPSDateTime' '-datetimeoriginal<GPSDateTime' \" + FRAMES_PATH + \" -fileorder filename -overwrite_original\")\n",
    "# rename images old names\n",
    "os.system(\"rename 's/_image_/_/g' \" + FRAMES_PATH + \"/*.jpg\")\n",
    "print(\"##############################################################################\")\n",
    "print(\"1 of 4 : EXPORT VIDEO & FRAME METADATA TO CSV\")\n",
    "print(\"##############################################################################\\n\")\n",
    "CSV_EXIFTOOL_FRAMES = METADATA_PATH + \"/metadata.csv\"\n",
    "'''\n",
    "CSV_EXIFTOOL_VIDEO =  METADATA_PATH + \"/csv_exiftool_video.csv\"\n",
    "export_frame_metadata =  \"exiftool -csv -fileorder filename \" + FRAMES_PATH + \" > \" + CSV_EXIFTOOL_FRAMES\n",
    "os.system(export_frame_metadata)\n",
    "\n",
    "# import frames metadata\n",
    "csv_exiftool_frames = pd.read_csv(CSV_EXIFTOOL_FRAMES)\n",
    "# import video metadata\n",
    "csv_exiftool_video = pd.read_csv(CSV_EXIFTOOL_VIDEO)\n",
    "# filter video metadata\n",
    "useful_video_metadata_names = ['LensSerialNumber', 'CameraSerialNumber', 'Model', 'AutoRotation', 'DigitalZoom', 'ProTune', 'WhiteBalance', 'Sharpness', 'ColorMode', 'MaximumShutterAngle', 'AutoISOMax', 'AutoISOMin', 'ExposureCompensation', 'Rate', 'FieldOfView', 'ElectronicImageStabilization', 'ImageWidth', 'ImageHeight', 'SourceImageHeight', 'XResolution', 'VideoFrameRate', 'ImageSize',\t'Megapixels', 'AvgBitrate']\n",
    "\n",
    "video_col_names = csv_exiftool_video.columns\n",
    "video_intersection_list = list(set(video_col_names) & set(useful_video_metadata_names))\n",
    "csv_exiftool_video = csv_exiftool_video[video_intersection_list]\n",
    "useful_video_metadata_values = csv_exiftool_video.iloc[0]\n",
    "# write video's metadata to frame csv\n",
    "for i in range(len(video_intersection_list)):\n",
    "    csv_exiftool_frames[video_intersection_list[i]] = video_intersection_list[i]\n",
    "# concat session_info csv and csv_exiftool_video csv\n",
    "result = pd.concat([session_info, csv_exiftool_video], axis=1)\n",
    "result.to_csv(SESSION_INFO_PATH, sep = ',', index=False)\n",
    "# then remove csv_exiftool_video csv\n",
    "os.remove(CSV_EXIFTOOL_VIDEO)\t\t\n",
    "'''\n",
    "\n",
    "print(\"##############################################################################\")\n",
    "print(\"4 of 4 : IMPORT EXIF METADATA\")\n",
    "print(\"##############################################################################\\n\")\n",
    "# save frame csv, before import metadata\n",
    "csv_exiftool_frames.to_csv(CSV_EXIFTOOL_FRAMES, index=False)\n",
    "#import_csv_metadata =  \"exiftool -csv=\" + CSV_EXIFTOOL + \" -fileorder filename \" + FRAMES_PATH + \" -c '%.9f'\"\n",
    "\n",
    "# once we have imported all metadata, remove useless columns from metadata csv\n",
    "col_names = csv_exiftool_frames.columns\n",
    "# EXIF metadata we want to keep, please check :\n",
    "# https://docs.google.com/spreadsheets/d/1iSKDvFrh-kP9wOU9bt9H7lcZKOnF7pe9n-8t15pOrmw/edit?usp=sharing\n",
    "keep_param_list = [\"ApertureValue\", \"Compression\", \"Contrast\", \"CreateDate\", \"DateCreated\", \"DateTimeDigitized\", \"DateTimeOriginal\", \"DigitalZoomRatio\", \"ExifImageHeight\", \"ExifImageWidth\", \n",
    "                    \"ExifToolVersion\", \"ExifVersion\", \"ExposureCompensation\", \"ExposureMode\", \"ExposureProgram\", \"FileName\", \"FileSize\", \"FileType\", \"FileTypeExtension\", \"FNumber\", \n",
    "                    \"FocalLength\", \"FocalLength35efl\", \"FocalLengthIn35mmFormat\", \"FOV\", \"GPSAltitude\", \"GPSAltitudeRef\", \"GPSDateTime\", \"GPSDate\", \"GPSTime\", \"GPSLatitude\", \"GPSLatitudeRef\", \"GPSLongitude\", \n",
    "                    \"GPSLongitudeRef\", \"GPSMapDatum\", \"GPSPosition\", \"GPSTimeStamp\", \"GPSRoll\", \"GPSPitch\", \"GPSTrack\",\"ImageHeight\", \"ImageWidth\", \"LightValue\", \"Make\", \"MaxApertureValue\", \n",
    "                    \"MaximumShutterAngle\", \"Megapixels\", \"MeteringMode\", \"MIMEType\", \"Model\", \"Saturation\", \"ScaleFactor35efl\", \"SceneCaptureType\", \"SceneType\", \"SensingMethod\", \"Sharpness\", \n",
    "                    \"ShutterSpeed\", \"Software\", \"SubSecDateTimeOriginal\", \"ThumbnailImage\", \"ThumbnailLength\", \"ThumbnailOffset\", \"WhiteBalance\", \"XResolution\", \"YResolution\"]\n",
    "# intersection between etadata we want to keep and EXIF meatdata\n",
    "intersection_list = list(set(col_names) & set(keep_param_list))\n",
    "# filter df\n",
    "csv_exiftool_frames = csv_exiftool_frames[intersection_list]\n",
    "# delete all empty columns\n",
    "csv_exiftool_frames.dropna(axis=1,inplace=True)\n",
    "# delete all zero columns\n",
    "csv_exiftool_frames = csv_exiftool_frames.loc[:, (csv_exiftool_frames != 0).any(axis=0)]\n",
    "# delete useless col\n",
    "#csv_exiftool_frames.drop(\"SubSecDateTimeOriginal_np\", axis=1, inplace=True)\n",
    "# save filtered frame csv, after import metadata\n",
    "csv_exiftool_frames.to_csv(CSV_EXIFTOOL_FRAMES, index=False)\n",
    "\n",
    "# end message\n",
    "os.system('spd-say \"lets gooooooooooooooooooooooooooooooo\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#RINEX_PATH = GPS_PATH + \"/reach_raw_202210211131_RINEX_3_03\"\n",
    "#RINEX_PATH = GPS_PATH + \"/reach_raw_202210231231_RINEX_3_03\"\n",
    "flag_gps = 0\n",
    "flag_bathy = 0\n",
    "flag_rinex = 0\n",
    "GPS_DEVICE_PATH = GPS_PATH + \"/DEVICE\"\n",
    "GPS_BASE_PATH = GPS_PATH + \"/BASE\"\n",
    "\n",
    "# check if we can do ppk\n",
    "for folder in os.listdir(GPS_DEVICE_PATH) :\n",
    "    # Look for the unzipped RINEX folder\n",
    "    if \"RINEX\" in folder and folder.endswith(\".zip\") :\n",
    "        flag_rinex = 1\n",
    "if os.path.exists(GPS_BASE_PATH):\n",
    "    if len(os.listdir(GPS_BASE_PATH)) != 0 and flag_rinex == 1:\n",
    "        flag_gps = 1\n",
    "        print(\"we can do PPK on our data !\")\n",
    "        LLH_PATH = ppk(SESSION_NAME, GPS_BASE_PATH, GPS_DEVICE_PATH, PPK_CONFIG_PATH, ppk_cfgs)\n",
    "\n",
    "# if we cannot do ppk\n",
    "if flag_gps == 0 :\n",
    "    print(\"we cannot do PPK on our data at the moment !\")\n",
    "    for folder in os.listdir(GPS_DEVICE_PATH) :\n",
    "        # if we have an unzipped LLH folder\n",
    "        if \"LLH\" in folder and not folder.endswith(\".zip\") :\n",
    "            LLH_FOLDER_PATH = GPS_DEVICE_PATH + \"/\" + folder\n",
    "            for file in os.listdir(LLH_FOLDER_PATH):\n",
    "                if file.endswith(\".LLH\"):\n",
    "                    flag_gps = 1\n",
    "                    LLH_PATH = LLH_FOLDER_PATH + \"/\" + file\n",
    "                    break\n",
    "        if \"LLH\" in folder and folder.endswith(\".zip\") and flag_gps == 0:\n",
    "            LLH_FOLDER_PATH = GPS_DEVICE_PATH + \"/\" + folder.replace(\".zip\", \"\")\n",
    "            with zipfile.ZipFile(GPS_DEVICE_PATH + \"/\" + folder, 'r') as zip_ref:\n",
    "                zip_ref.extractall(LLH_FOLDER_PATH)\n",
    "            for file in os.listdir(LLH_FOLDER_PATH):\n",
    "                if file.endswith(\".LLH\"):\n",
    "                    flag_gps = 1\n",
    "                    LLH_PATH = LLH_FOLDER_PATH + \"/\" + file\n",
    "                    break            \n",
    "        \n",
    "# check if we have a navigation file\n",
    "if flag_gps == 1 :\n",
    "    print(os.path.join(\"we have the following navigation file : \", LLH_PATH))\n",
    "else :\n",
    "    print(\"we do not have a navigation file\")\n",
    "\n",
    "# if we have a navigation file\n",
    "if flag_gps :\n",
    "    TXT_PATH  = llh_to_txt(LLH_PATH)\n",
    "    print(\"the NEW navigation file will be : \", TXT_PATH)\n",
    "\n",
    "if os.path.isdir(BATHY_PATH) :\n",
    "    # for each file in the videos folder\n",
    "    for file in os.listdir(BATHY_PATH):\n",
    "        if file.endswith(\"bathy_preproc.csv\"):\n",
    "            flag_bathy = 1\n",
    "            BATHY_PREPOC_PATH = BATHY_PATH + \"/\" + file\n",
    "            CSV_BATHY_PREPOC = bathy_preproc_to_txt(BATHY_PREPOC_PATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# prova per sunnith roll pitch yaw\n",
    "my_time = time_first_frame\n",
    "DIR = \"/media/mcontini/Data_Paper_Hard_Disk/session_2022_10_21_aldabra_DUBOIS_plancha_body_v1A_01/DCIM/videos/frames/\"\n",
    "if os.path.isdir(BATHY_PATH) :\n",
    "    # for each file in the videos folder\n",
    "    for file in os.listdir(BATHY_PATH):\n",
    "        if file.endswith(\"bathy_preproc.csv\"):\n",
    "            flag_bathy = 1\n",
    "            BATHY_PREPOC_PATH = BATHY_PATH + \"/\" + file\n",
    "txt_path = bathy_preproc_to_txt(BATHY_PREPOC_PATH)\n",
    "txt_path = txt_path[0]\n",
    "print(\"##############################################################################\")\n",
    "print(\"1 of 6 : We are setting the following date and time \", my_time, \" to all the frames\")\n",
    "print(\"##############################################################################\\n\")\n",
    "change_date_and_time =  \"exiftool -m '-SubSecDateTimeOriginal= \" + my_time + \"' \" + DIR + \" -fileorder filename -overwrite_original\"\n",
    "os.system(change_date_and_time)\n",
    "#print_date_and_time = \"exiftool -m -SubSecDateTimeOriginal \" + DIR + \" -fileorder filename\"\n",
    "#os.system(print_date_and_time)\n",
    "# First, copy SubSecDateTimeOriginal to XMP:DateTimeOriginal\n",
    "print(\"##############################################################################\")\n",
    "print(\"2 of 6 : We are copying the EXIF time to XMP times\")\n",
    "print(\"##############################################################################\\n\")\n",
    "change_tag_date_and_time = \"exiftool -m '-XMP:DateTimeOriginal<SubSecDateTimeOriginal' \" + DIR + \" -fileorder filename -overwrite_original\"\n",
    "os.system(change_tag_date_and_time)\n",
    "# Then, run your microsecond change on the XMP:DateTimeOriginal.  This works because XMP timestamps are more flexible than EXIF timestamps.\n",
    "print(\"##############################################################################\")\n",
    "print(\"3 of 6 : We are incrementing the XMP time\")\n",
    "print(\"##############################################################################\\n\")\n",
    "increment_time = \"exiftool -m '-XMP:DateTimeOriginal+<0:0:${filesequence;$_*=\" + str(delta_time) + \"}' \" + DIR + \" -fileorder filename -overwrite_original\"\n",
    "os.system(increment_time)\n",
    "#print_date_and_time = \"exiftool -m -XMP:DateTimeOriginal \" + DIR + \" -fileorder filename\"\n",
    "#os.system(print_date_and_time)\n",
    "# Finally, copy the XMP:DateTimeOriginal to the other timestamps.\n",
    "print(\"##############################################################################\")\n",
    "print(\"4 of 6 : Weare updating EXIF times\")\n",
    "print(\"##############################################################################\\n\")\n",
    "update_date_and_time =  \"exiftool -m '-SubSecTimeOriginal<XMP:DateTimeOriginal'  '-SubSecTime<XMP:DateTimeOriginal' '-SubSecTimeDigitized<XMP:DateTimeOriginal' '-datetimeoriginal<XMP:DateTimeOriginal' \" + DIR + \" -fileorder filename -overwrite_original\"\n",
    "os.system(update_date_and_time)\n",
    "# Add metadata in DIR_GPS to images in DIR. Since Roll and Pitch are not standard tags, we need to create them in the config file. Then syncronize Latitude, Longitude, Yaw, Roll and Pitch \n",
    "# thanks to the geotag command, Yaw is stored in the XMP-exif:GPSTrack tag. \n",
    "# For more infromations, plase refer to :\n",
    "# https://exiftool.org/forum/index.php?topic=14155.0\n",
    "print(\"##############################################################################\")\n",
    "print(\"5 of 6 : We are adding XMP metadata to frames\")\n",
    "print(\"##############################################################################\\n\")\n",
    "write_gps_roll_pitch_yaw = \"exiftool -config \" + exiftool_config_path + \" -m -geotag \" + txt_path + \" '-xmp:geotime<${XMP:DateTimeOriginal}+00:00' \" + DIR + \" -overwrite_original\"\n",
    "os.system(write_gps_roll_pitch_yaw)\n",
    "print(\"##############################################################################\")\n",
    "print(\"6 of 6 : We are copying XMP metadata to EXIF metadata\")\n",
    "print(\"##############################################################################\\n\")\n",
    "update_geotag_metadata = \"exiftool -r -overwrite_original '-gps:all<xmp-exif:all' \" + DIR\n",
    "os.system(update_geotag_metadata)\n",
    "print(\"##############################################################################\")\n",
    "print(\"############################ FINISHED ########################################\")\n",
    "print(\"##############################################################################\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE METADATA MATISSE 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##################\")\n",
    "print(\"image metadata will be written in a few seconds\")\n",
    "# create metadata csv file\n",
    "MATISSE_PATH = SESSION_PATH + \"/METADATA/\" + SESSION_NAME + \"_matisse_metadata.txt\"\n",
    "\n",
    "os.system(\"exiftool -T -n -csv -ext jpeg \" + FRAMES_PATH + \"> \" + MATISSE_PATH)\n",
    "# df with all EXIF metadata\n",
    "df = pd.read_csv(METADATA_PATH)  \n",
    "col_names = df.columns\n",
    "# EXIF metadata we want to keep, please check :\n",
    "# https://docs.google.com/spreadsheets/d/1iSKDvFrh-kP9wOU9bt9H7lcZKOnF7pe9n-8t15pOrmw/edit?usp=sharing\n",
    "keep_param_list = [ \"GPSDateTime\", \"GPSAltitude\", \"GPSLatitude\", \"GPSLongitude\", \n",
    "                     \"GPSRoll\", \"GPSPitch\", \"GPSTrack\"]\n",
    "# intersection between etadata we want to keep and EXIF meatdata\n",
    "intersection_list = list(set(col_names) & set(keep_param_list))\n",
    "# filter df\n",
    "df = df[intersection_list]\n",
    "\n",
    "# Standardize time to have 3 millisecond digits\n",
    "for i in range(0, len(df[\"GPSDateTime\"])) :\n",
    "    time = df[\"GPSDateTime\"][i]\n",
    "    # print(time)\n",
    "    # If there is only one millisecond digit, add two '0'\n",
    "    if len(time) == 21 :\n",
    "        df[\"GPSDateTime\"][i] = df[\"GPSDateTime\"][i] + \"00\"\n",
    "    # If there are two millisecond digit, add one '0'\n",
    "    elif len(time) == 22 :\n",
    "        df[\"GPSDateTime\"][i] = df[\"GPSDateTime\"][i] + \"0\"\n",
    "\n",
    "\n",
    "# Divide date and time into 2 columns\n",
    "df[[\"date_yyyy/MM/dd\", \"time\"]] = df[\"GPSDateTime\"].str.split(\" \", 1, expand = True)\n",
    "df.drop(\"GPSDateTime\", inplace = True, axis = 1)\n",
    "\n",
    "# Rename the columns for Matisse file\n",
    "df.rename(columns = {\"GPSAltitude\": \"depth\", \"GPSLatitude\": \"latitude\", \"GPSLongitude\": \"longitude\", \"GPSRoll\": \"roll\", \"GPSPitch\": \"pitch\", \"GPSTrack\": \"heading\"}, inplace = True)\n",
    "\n",
    "# Convert date from yyyy:MM:dd to yyyy/MM/dd (Matisse format)\n",
    "df[\"date_yyyy/MM/dd\"] = df[\"date_yyyy/MM/dd\"].str.replace(\":\", \"/\")\n",
    "\n",
    "# Sort the dataframe by time order\n",
    "df = df.sort_values(by = \"time\", ascending = True)\n",
    "\n",
    "\n",
    "# delete files that are not valid images\n",
    "#df = df[df[\"FileName\"].astype(str).str.startswith(\"session\")]\n",
    "# save df\n",
    "#df.to_csv(METADATA_PATH)\n",
    "#print(METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Matisse metadata into a csv file\n",
    "\n",
    "df.to_csv(MATISSE_PATH, header = True, sep = \",\", index = False)   \n",
    "line = \"##\" \n",
    "with open(MATISSE_PATH, \"r+\") as file: \n",
    " file_data = file.read() \n",
    " file.seek(0, 0) \n",
    " file.write(line + file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RENAMING FRAMES FOR MATISSE 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the frames in another folder\n",
    "\n",
    "print(\"##################\")\n",
    "print(\"Creating a new folder called 'frames_matisse' and copying all frames inside\")\n",
    "\n",
    "FRAMES_PATH_M = VIDEOS_PATH + \"/frames_matisse\"\n",
    "\n",
    "# If it does not already exist, create a new folder\n",
    "if not os.path.exists(FRAMES_PATH_M):\n",
    "    os.makedirs(FRAMES_PATH_M)\n",
    "\n",
    "# Copy and paste all frames\n",
    "os.system(\"cp -r \" + FRAMES_PATH + \"/* \" + FRAMES_PATH_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##################\")\n",
    "print(\"Getting the right name for all frames in Matisse format\")\n",
    "\n",
    "# Transform to YYYYMMDDThhmmss.fffZ Matisse format in order to rename the frames\n",
    "df = pd.read_csv(MATISSE_PATH)  \n",
    "FrameName = df[\"date_yyyy/MM/dd\"] + \"T\" + df[\"time\"] + \"Z\"    \n",
    "FrameName = FrameName.str.replace(\"/\", \"\")\n",
    "FrameName = FrameName.str.replace(\":\", \"\")\n",
    "\n",
    "# Create a function to correctly sort the frames (ex : 1, 2, 10 instead of Python usually 1, 10, 2)\n",
    "import re\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "\n",
    "print(\"Renaming all frames...\")\n",
    "\n",
    "# Rename each frame following the time order\n",
    "folder = FRAMES_PATH_M\n",
    "\n",
    "for count, oldname in (enumerate(sorted_alphanumeric(os.listdir(folder)))) :\n",
    "    oldpath =f\"{folder}/{oldname}\"\n",
    "    newname = FrameName[count] + \".jpeg\"\n",
    "    newpath = f\"{folder}/{newname}\"\n",
    "    os.rename(oldpath, newpath)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN PREDICTIONS AND EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.coco as fouc\n",
    "import fiftyone.utils.data as foud\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/mcontini/Desktop/PhD/Git_projects/fiftyone/seatizen/dataset_engineering/')\n",
    "from fo_dataset_creation import create_or_load_dataset\n",
    "from custom_csv_exporter import CSVImageClassificationDatasetExporter\n",
    "from darwincore import DarwinCoreExporter, match_taxa_in_worms_database\n",
    "\n",
    "sys.path.insert(0, '/home/mcontini/Desktop/PhD/Git_projects/fiftyone/segmentation_predictions/')\n",
    "from add_predictions import build_predictor, add_predictions_to_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print and clean all permanent datasets\n",
    "print(fo.list_datasets())\n",
    "for i in fo.list_datasets() :\n",
    "    dataset = fo.load_dataset(i)\n",
    "    dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set paths and variables\n",
    "dataset_name = SESSION_NAME\n",
    "dataset_name = \"/home/mcontini/Desktop/Ifremer/Seatizen/data/session_2022_10_20_aldabra_ARM01_plancha_body_v1A_01_*0/\"\n",
    "SESSION_PATH = dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_path = FRAMES_PATH\n",
    "#metadata_df = METADATA_PATH\n",
    "metadata_df = dataset_name + \"metadata.csv\"\n",
    "images_path = dataset_name\n",
    "checkpoint_path = \"/home/mcontini/Desktop/Ifremer/Seatizen/Git/seatizen/instance_segmentation/outputs/2022-06-15_LR_0.00025_BATCH_2_ITER_150000_1655286559.4163053/model_final.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The current device is : \", device)\n",
    "thing_classes = [\n",
    "    'sea cucumber', 'Syringodium isoetifolium', 'Sand', 'Scrap', \n",
    "    'Rock', 'Trample', 'Waste', 'Acropore Branched', \n",
    "    'Acropore Digitised', 'Acropore Sub-massive', 'Acropore Tabular', 'No acropore Branched', \n",
    "    'No acropore Encrusting', 'No acropore Foliaceous', 'No acropore Massive', 'No acropore Sub massive', \n",
    "    'No acropore Solitary', 'Millepore', 'Dead coral', 'fish', \n",
    "    'Sponge', 'Sea urchins', 'Clam', 'Algae Limestone', \n",
    "    'Algae Drawn up', 'Algae assembly', 'Soft coral', 'Living Coral', \n",
    "    'Bleached coral'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create or load 51 dataset\n",
    "dataset = create_or_load_dataset(\n",
    "    dataset_name=dataset_name, \n",
    "    dataset_type='unlabeled', \n",
    "    images_path=images_path)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add metadata to the fiftyone dataset\n",
    "metadata_df = pd.read_csv(metadata_df, low_memory = False, na_values = ['-', ' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in tqdm(dataset):\n",
    "\n",
    "    image_metadata = metadata_df[metadata_df['FileName']==os.path.basename(sample['filepath'])] # select the metadata matching with the sample\n",
    "    \n",
    "    if not image_metadata.empty: # make sure the metadata row was not empty in case the sample did not had metadata associated in the CSV file\n",
    "\n",
    "        # if lat and long are filled in, add the GPS position as a geolocation field in the 51 dataset\n",
    "        if image_metadata['GPSLatitude'].hasnans==False and image_metadata['GPSLongitude'].hasnans==False: \n",
    "            sample['locations'] = fo.GeoLocation(point=[image_metadata['GPSLatitude'].iloc[0], image_metadata['GPSLongitude'].iloc[0]])\n",
    "            sample.save()\n",
    "\n",
    "        # if the datetime from the RTK is filled in, add it as a datetime. Otherwise, use the DateTimeOriginal column (if provided).\n",
    "        if image_metadata['SubSecDateTimeOriginal'].hasnans and image_metadata['DateTimeOriginal'].hasnans==False :\n",
    "            date_field_used = 'DateTimeOriginal'\n",
    "            sample['datetime'] = datetime.strptime(str(image_metadata['DateTimeOriginal'].iloc[0]), '%Y:%m:%d %H:%M:%S')\n",
    "            sample.save()\n",
    "        elif image_metadata['SubSecDateTimeOriginal'].hasnans==False:\n",
    "            date_field_used = 'SubSecDateTimeOriginal'\n",
    "            sample['datetime'] = datetime.strptime(str(image_metadata['SubSecDateTimeOriginal'].iloc[0]), '%Y:%m:%d %H:%M:%S.%f')\n",
    "            sample.save()\n",
    "\n",
    "        # Add each metadata variable (except fields already added in the if statements before) to the sample.\n",
    "        for exif_variable in image_metadata.drop([ 'GPSLatitude', 'GPSLongitude', date_field_used], axis=1):\n",
    "            if image_metadata[exif_variable].hasnans==False:\n",
    "                sample[exif_variable] = image_metadata[exif_variable].iloc[0]\n",
    "                sample.save()\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict species on images\n",
    "predictor = build_predictor(checkpoint_path, device, len(thing_classes), \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = add_predictions_to_dataset(\n",
    "    dataset, \n",
    "    predictor, \n",
    "    device, \n",
    "    thing_classes, \n",
    "    predictions_field='nms_predictions', \n",
    "    nms_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Launch fiftyone to admire the results\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export results in folders\n",
    "export_results_directory = SESSION_PATH + \"/METADATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Export in darwincore\n",
    "fields_darwincore_mapping = '/home/mcontini/Desktop/Ifremer/Seatizen/Git/seatizen/fiftyone/import_export_dataset/darwincore_mapping/fields_darwincore_mapping.yaml'\n",
    "taxon_mapping = '/home/mcontini/Desktop/Ifremer/Seatizen/Git/seatizen/fiftyone/import_export_dataset/darwincore_mapping/taxon_mapping_matteo.yaml'\n",
    "datetime_mapping = '/home/mcontini/Desktop/Ifremer/Seatizen/Git/seatizen/fiftyone/import_export_dataset/darwincore_mapping/darwincore_datetime_mapping.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only with internet\n",
    "with open(taxon_mapping) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "scinames = []\n",
    "for label in config['CLASSES']:\n",
    "    scinames.append(config['CLASSES'][label]['taxon_research'])\n",
    "\n",
    "taxon_information_df = match_taxa_in_worms_database(scinames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_information_df = pd.read_csv(\"/home/mcontini/Desktop/Ifremer/Monaco/Monaco_scripts/taxon_information_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darwincore_exporter = DarwinCoreExporter(\n",
    "    os.path.join(export_results_directory, 'Darwincore_export'), \n",
    "    fields_darwincore_mapping, \n",
    "    taxon_mapping, \n",
    "    datetime_mapping,\n",
    "    taxon_information_df,\n",
    "    \"polylines\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.export(\n",
    "    dataset_exporter=darwincore_exporter,\n",
    "    label_field = 'nms_predictions',\n",
    "    export_media='manifest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Export coco\n",
    "coco_exporter = fouc.COCODetectionDatasetExporter(\n",
    "    export_dir=os.path.join(export_results_directory, 'COCODataset_export'),\n",
    "    data_path='manifest.json',\n",
    "    labels_path='coco_labels.json',\n",
    "    export_media='manifest',\n",
    "    classes=dataset.distinct('nms_predictions.polylines.label'),\n",
    "    tolerance=2, \n",
    "    extra_attrs=False)\n",
    "dataset.export(dataset_exporter=coco_exporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Export the FiftyoneDataset in json\n",
    "dataset.export(\n",
    "    export_dir=os.path.join(export_results_directory, 'FiftyoneDataset_export'),\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=False,\n",
    "    label_field='nms_preidctions',\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.load_dataset('seatizen')\n",
    "print(dataset)\n",
    "fo.launch_app(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3423a53466853ad8c64a24506a1f2333dbd14a87e6de1491ad93714aa7283d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
